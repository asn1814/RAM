dataset: openassistant2_preference_llama3
max_seq_len: 8192
max_num_tokens: 8192
example_shuffle_window: 10000
batch_shuffle_window: 1000
num_prefetch: 4
model: checkpoint_step_100
resume_checkpoint_dir: dummy/path/to/checkpoints/dir
dtype: bfloat16
data_parallelism: fsdp
fsdp_wrap_granularity: layer
fsdp_reshard_after_forward: true
tensor_parallel_size: 1
activation_checkpointing: true
torch_compile: false
optimizer: adamw
optimizer_config: 
  _type_: fairseq2.optim.factory.AdamWConfig
  lr: 5.5e-06
  betas:
  - 0.9
  - 0.95
  weight_decay: 0.1
lr_scheduler: cosine-annealing
lr_scheduler_config:
  _type_: fairseq2.optim.lr_scheduler.factory.CosineAnnealingLRConfig
  num_warmup_steps: 0
  final_lr: 1.1e-06
gradient_accumulation: 4
max_gradient_norm: null
fp16_loss_scale:
- 128.0
- 0.0001
max_num_steps: 800
max_num_data_epochs: 5
checkpoint_every_n_data_epochs: 1
keep_last_n_checkpoints: 5
publish_metrics_every_n_steps: 5
seed: 1814
profile: null
monitored_gang: false
anomaly_detection: false
criterion: dpo
criterion_config:
  _type_: fairseq2.recipes.lm.preference_finetune.dpo.DpoConfig
  reference_model: checkpoint_step_100
  reference_dtype: bfloat16
  reference_tensor_parallel_size: 1
  beta: 0.1
  nll_scale: 0.0
